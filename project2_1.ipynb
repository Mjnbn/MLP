{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت کلاس \n",
    "activation \n",
    "را میسازیم که همه تابع های فعال ساز و متود مشتق آن هارا در این کلاس قرار می دهیم\n",
    "و در ابتدا تابع سازنده این کلاس را می نویسیمو در ادامه ازین کلاس در هر نود که ساختیم ابجکت میسازسم و بسته به این که تابع فعال ساز آن لایه را چه وارد کرده ایم متود فعال ساز آن لایه را فراخوانی می کنیم از این کلاس استفاده می کنیم . \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Activation:\n",
    "   # with out use np\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    def sigmoid_prime(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    def relu(self, x):\n",
    "        return max(0.1 * x, x)\n",
    "    def relu_prime(self, x):\n",
    "        return 1 if x > 0 else 0.1\n",
    "    def step(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "    def step_prime(self, x):\n",
    "        return  1\n",
    "    \n",
    "    def relu_prime(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "    def tanh(self, x):\n",
    "        return math.tanh(x)\n",
    "    def tanh_prime(self, x):\n",
    "        return 1 - math.tanh(x) ** 2\n",
    "    def __call__(self, x):\n",
    "        return self.sigmoid(x)\n",
    "    def prime(self, x):\n",
    "        return self.sigmoid_prime(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت کلاس نود را پیاده سازی می کنیم که این نود ها همان نود های شبکه عصبی هستند که در قسمت \n",
    "init\n",
    "مقدار های اولیه آن ها را می دهیم و \n",
    "weight , bias\n",
    "را که در اصل همان وزن هایی است که این نود با ضرب شدن و در نهایت جمع شدن در آن به وجود می آید و \n",
    "output\n",
    "همان خروجی نهایی هر نورون پس از عبور از فعال ساز است و \n",
    "input\n",
    "ورودوی این نورون یا همان خروجی لایه قبل است و \n",
    "nextLweights\n",
    "هم وزن های لایه بعد است که از این نورون به نود های لایه بعدی وصل شده است \n",
    "و\n",
    "nextdeltas\n",
    "هم ذلتا های لایه بعد است که برای پیدا کردن دلتای  این لایه به آن ها نیاز داریم\n",
    "و\n",
    "biasprime , weightprime\n",
    "هم مشتق وزن ها و بایاس این نورون است که برای اپدیت\n",
    "به این ها نیاز داریم\n",
    "و\n",
    "Activatioin\n",
    "که کلاس است را در هر نود ابتدا تشکیل می دهیم که از تابع های آن استفاده کنیم\n",
    "و\n",
    "z\n",
    "هم خروجی قبل از رد شدن از تابع فعال ساز است \n",
    "و\n",
    "activation\n",
    "تابع این نورون است که تعیین می کنیم که چه تابعی باشد و همچنین \n",
    "activationprime\n",
    "هم با تئجه به این که گفتع شده چه تابعی فعال ساز باشد مشتق آن هم مشخص می کنیم.\n",
    "تابع \n",
    "forward\n",
    "ورودی این نورون که مشخص شده است \n",
    "و ورودی را در بردار وزن ضرب داخلی می کنیم که برابر\n",
    "z\n",
    "است و\n",
    "از آن تابع فعال ساز که مشخص کردیم میگیریم و \n",
    "output \n",
    " را به ما می دهد.\n",
    "و تابع های get\n",
    "را می نویسیم که اگر خواستیم دریافت کنیم پارامتر های مورد نظر را بتوانیم.\n",
    "update\n",
    "به این صورت عمل می کند که مقدار وزن یا بایاس مورد نظر را منهای مشتق ان ضرب در \n",
    "learning rate \n",
    "می کنیم .\n",
    "قسمت \n",
    "compute delta\n",
    "از فرمول حساب دلتا و با داشتن \n",
    "وزن ها و دلتای بعدی می توانیم دلتای این نورون را محاسبه کنیم.\n",
    "و محاسبه دلتا برای لایه اخر متفاوت است برای همین دو حالت می نویسیم.\n",
    "در \n",
    "make_weights_bias_prime\n",
    "مشتق نسبت به وزن ها و بایاس را باداشتن دلتا و ورودی این نورون که همان خروجی لایه قبل است محاسبه می کنیم.\n",
    "و بقیه تابع ها \n",
    "set \n",
    "هستند که برای ست کردن پارامتر ها استفاده میکنیم.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Node:\n",
    "    # with random initialization \n",
    "    def __init__(self, lastlayer_size, nextlayer_size, activation):\n",
    "        #random initialization between 0 and 5\n",
    "        self.weights = np.random.rand(lastlayer_size)\n",
    "        self.bias = np.random.rand()\n",
    "        self.output = 0\n",
    "        self.delta = 0\n",
    "        self.input = [0] * lastlayer_size\n",
    "        self.nextLweights = [0] * nextlayer_size\n",
    "        self.nextdeltas = [0] * nextlayer_size\n",
    "        self.biasprime = 0\n",
    "        self.weightsprime = [0] * lastlayer_size\n",
    "        self.activation = activation\n",
    "        self.z = 0\n",
    "        self.Activation_functions = Activation()\n",
    "\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.activation_function = self.Activation_functions.sigmoid\n",
    "        elif self.activation == \"relu\":\n",
    "            self.activation_function = self.Activation_functions.relu\n",
    "        elif self.activation == \"tanh\":\n",
    "            self.activation_function = self.Activation_functions.tanh\n",
    "        elif self.activation == \"step\":\n",
    "            self.activation_function = self.Activation_functions.step\n",
    "        else:\n",
    "            self.activation_function = self.Activation_functions.sigmoid\n",
    "        \n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            self.activation_function_prime = self.Activation_functions.sigmoid_prime\n",
    "        elif self.activation == \"relu\":\n",
    "            self.activation_function_prime = self.Activation_functions.relu_prime\n",
    "        elif self.activation == \"tanh\":\n",
    "            self.activation_function_prime = self.Activation_functions.tanh_prime\n",
    "        elif self.activation == \"step\":\n",
    "            self.activation_function_prime =self.Activation_functions.step_prime\n",
    "        else:\n",
    "            self.activation_function_prime = self.Activation_functions.sigmoid_prime\n",
    "\n",
    "\n",
    "        # make output in this part\n",
    "    def forward(self, x,isoutputlayer = False ):\n",
    "\n",
    "\n",
    "        self.input = x\n",
    "        self.z = sum([i * j for i, j in zip(self.weights, x)]) + self.bias\n",
    "        if isoutputlayer :\n",
    "            self.output =self.activation_function(self.z)\n",
    "        else:\n",
    "            self.output = self.activation_function(self.z)\n",
    "        return self.output\n",
    "\n",
    "    def getz(self):\n",
    "        return self.z\n",
    "\n",
    "    def getdelta(self):\n",
    "        return self.delta\n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.weights = [i - lr * j for i, j in zip(self.weights, self.weightsprime)]\n",
    "        self.bias = self.bias - lr * self.biasprime\n",
    "\n",
    "\n",
    "        # print the weightsprime\n",
    "        return self.biasprime\n",
    "    \n",
    "    def compute_delta(self, real_y,isoutputlayer = False ):\n",
    "        # nextweights dot nextdeltas multiply by sigmoid prime\n",
    "     \n",
    "        if isoutputlayer:\n",
    "            # mse prime\n",
    "            self.delta = 2*(self.output - real_y) * self.activation_function_prime(self.z)\n",
    "        else:\n",
    "            self.delta = sum([i * j for i, j in zip(self.nextLweights, self.nextdeltas)]) * self.activation_function_prime(self.z)\n",
    "          \n",
    "            \n",
    "    \n",
    "    def make_weights_bias_prime(self):\n",
    "        self.weightsprime = [i * self.delta for i in self.input]\n",
    "        self.biasprime = self.delta\n",
    "     \n",
    "    def setInputs(self, inputs):\n",
    "        self.input = inputs\n",
    "    \n",
    "    def setNextLweights(self, weights):\n",
    "        self.nextLweights = weights\n",
    "    \n",
    "    def setNextLdeltas(self, deltas):\n",
    "        self.nextdeltas = deltas\n",
    "    \n",
    "    def getweights_prime(self):\n",
    "        return self.weightsprime\n",
    "    def getbias_prime(self):\n",
    "        return self.biasprime\n",
    "    \n",
    "\n",
    "    def setweights_prime(self,weights_prime):\n",
    "        self.weightsprime = weights_prime\n",
    "    def setbias_prime(self,bias_prime):\n",
    "        self.biasprime = bias_prime\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در این قسمت کلاس لایه را پیاده سازی می کنیم که این کلاس در قسمت مقدار دهی اولیه\n",
    "یک لیست از نود ها دارد که که همان نود های است مه در آن لایه قرار دارند و ساز این لایه که تعداد نورون های این لایه است را ورودی گرفته ایم و \n",
    "در ابتدا به همان تعداد ابجکت نود میسازیم \n",
    "و \n",
    "outputs\n",
    " که همان تمام خروجی های نود های این لایه است را در این لیست ذخیره میکنیم \n",
    " deltas\n",
    " هم تمام دلتا ها این نود هاست که در لیست نگه می داریم\n",
    " و\n",
    " forward\n",
    " آن به این صورت است که تمام نود های این لایه را \n",
    " forward \n",
    "  میکنیم .\n",
    "  و\n",
    "  setinputs\n",
    "  هم ورودی تمام نود هارا برابر\n",
    "  x\n",
    "  قرار می دهیم که خروجی لایه قبل است.\n",
    "  و\n",
    "  getdeltas\n",
    "  هم تمام دلتا های نود های این لایه را در یم لیست به ما بر میگرداند.\n",
    "  getweights_related_i\n",
    "  این تابع می خواهد به ازای تمام نود ها وزن \n",
    "  i\n",
    "  ام آن را در یک لیست برگرداند چون\n",
    "  چون برای لایه قبل برای پیدا کردن وزن هایی که به یک نورون خاص در لایه قبل وصل هستند یا همان\n",
    "  nextweights\n",
    "  لایه قبل به آن نیاز داریم.\n",
    "  compute_deltas\n",
    "  ,\n",
    "  update_weights\n",
    "  و\n",
    "  compute_weights_bias_prime\n",
    "  هم برای هر نورون همین تابع را در هر نورون صدا می زند.\n",
    "  set_nextLweights_and_deltas\n",
    "  همانطور که گفتیم با استفاده از تابع \n",
    "  getweights_related_i\n",
    "  برای هر نورون با داشتن ابجکت لایه بعد می توانیم وزن های لایه بعد و دلتای لایه بعد را برای هر نورون محاسبه کنیم..\n",
    "  set_weights_prime\n",
    "  و\n",
    "  set_bias_prime\n",
    "  هم برای هر نورون این تابع را صدا میزند.\n",
    "\n",
    "  bacward\n",
    "  در لایه اول \n",
    "  از\n",
    "  compute_deltas\n",
    "  شروع می کنیم و برای هر نورون این تابع را صدا میزنیم و سپس\n",
    "  compute_weights_bias_prime\n",
    "  را صدا میزنیم و برای هر نورون این تابع را صدا میزنیم و مشتق نسبت به وزن ها و بایاس هارا در ایان لایه پیدا می کنیم.\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \n",
    "    def __init__(self,size,lastlayer_size,nextlayer_size,activation):\n",
    "        self.nodes = [Node(lastlayer_size,nextlayer_size,activation) for i in range(size)]\n",
    "        self.ouuputs = []\n",
    "        self.deltas = []\n",
    "    def forward(self,x,isoutputlayer = False):\n",
    "        self.outputs = [node.forward(x,isoutputlayer) for node in self.nodes]\n",
    "        return self.outputs\n",
    "    def set_inputs(self,x,isoutputlayer = False ):\n",
    "        for i in range(len(self.nodes)):\n",
    "            self.nodes[i].set_input(x)\n",
    "\n",
    "    def getdeltas(self):\n",
    "        self.deltas = [node.getdelta() for node in self.nodes]\n",
    "        return self.deltas\n",
    "    \n",
    "    def getweights_related_i(self,i):\n",
    "        return [node.weights[i] for node in self.nodes]\n",
    "    \n",
    "\n",
    "    def compute_deltas(self,real_y = 0,isoutputlayer = False):\n",
    "        for i in range(len(self.nodes)):\n",
    "            self.nodes[i].compute_delta(real_y ,isoutputlayer)\n",
    "\n",
    "    def compute_weights_bias_prime(self):\n",
    "        for i in range(len(self.nodes)):\n",
    "            self.nodes[i].make_weights_bias_prime()\n",
    "\n",
    "    def update_weights(self,lr):\n",
    "        weightsprime = []\n",
    "        for i in range(len(self.nodes)):\n",
    "            weightsprime.append(self.nodes[i].update(lr))\n",
    "        # print(\"weightsprime\",weightsprime)\n",
    "\n",
    "    def set_nextLweights_and_deltas(self,nextlayer):\n",
    "        deltas = nextlayer.getdeltas()\n",
    "        for i in range(len(self.nodes)):\n",
    "            self.nodes[i].setNextLweights(nextlayer.getweights_related_i(i))\n",
    "            self.nodes[i].setNextLdeltas(deltas)\n",
    "           \n",
    "    \n",
    "    def backward(self,real_y = 0,isoutputlayer = False):\n",
    "        self.compute_deltas(real_y ,isoutputlayer)\n",
    "        self.compute_weights_bias_prime()\n",
    "        # self.update_weights(lr=1)\n",
    "\n",
    "\n",
    "        \n",
    "    def getweights_prime(self):\n",
    "        weightsprime = []\n",
    "        for i in range(len(self.nodes)):\n",
    "            weightsprime.append(self.nodes[i].getweights_prime())\n",
    "        return weightsprime\n",
    "    def getbias_prime(self):\n",
    "        biasprime = []\n",
    "        for i in range(len(self.nodes)):\n",
    "            biasprime.append(self.nodes[i].getbias_prime())\n",
    "        return biasprime\n",
    "    def set_weights_prime(self,weights_prime):\n",
    "        for i in range(len(self.nodes)):\n",
    "            self.nodes[i].setweights_prime(weights_prime[i])\n",
    "    def set_bias_prime(self,bias_prime):\n",
    "        for i in range(len(self.nodes)):\n",
    "            self.nodes[i].setbias_prime(bias_prime[i])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "حال در این قسمت کلاس \n",
    "Mlp\n",
    "را پیاده سازی می کنیم که در ابتدا لیستی از ابجکت های لایه هاست که به تعداد لایه ها لایه می سازیم و تعداد نورون لایه قبل و بعد را به آن میدهیم و همچنین اسم تابع فعال ساز مخصوص این لایه را.\n",
    "همچنین یک لایه با یک نورون برای لایه نهاییکه مخفی نیست هم می سازیم.\n",
    "ورودی را هم به لایه مخفی اول یا اگر لایه مخفی نداشت به لایه اخر می دهیم.\n",
    "تابع \n",
    "forward\n",
    "هم به این صورت است که تمام لایه ها را\n",
    "forward\n",
    "می کنیم.\n",
    "و\n",
    "خروجی هر لایه را به عنوان ورودی لایه بعدی می دهد.\n",
    "و\n",
    "call\n",
    "همان \n",
    "forward\n",
    "را صدا می زند.\n",
    "و تابع \n",
    "parametes\n",
    "هم تمام وزن ها و بایاس ها را بر می گرداند.\n",
    "تابع\n",
    "backward\n",
    "ابتدا \n",
    "bacward\n",
    "را از لایه اخر بر روی این لایه میزنذ تا م شتق نسبت به لایه را پیدا کندو با استفاده از تابع \n",
    "set_nextLweights_and_deltas\n",
    "وزن ها و دلتای این لایه مورد نظر که در \n",
    "for\n",
    "روی آن هستیم را به لایه بعد منتقل می کنیم.\n",
    "و\n",
    "loss\n",
    "که همان تابع خطا است را محاسبه می کنیم.\n",
    "با استفاده از فرمول مورد نظر و  گرفتن وردی واقعی.\n",
    "set_weightsLi\n",
    "و\n",
    "set_biasLi\n",
    "هم برای هر لایه این تابع را صدا میزنیم.\n",
    "و\n",
    "weight , bias\n",
    "لایه مورد نظر که با شمار هان مشخص شده به هر نورون آن وارد می کنیم.\n",
    "و در ادامه تابع های \n",
    "get \n",
    "موارد مورد نظر را از تمام لایه ها گرفته و در یک لیست قرار می دهد و به ما بر میگرداند.\n",
    "و\n",
    "set\n",
    "ها هم همین صورت این چنین لیستی را گرفته و برای همه لای ها مقادیر مرود نظر را \n",
    "set \n",
    " می کند.\n",
    " update\n",
    "    هم برای هر لایه این تابع را صدا میزنیم.و \n",
    "learning rate\n",
    "را منتقل می کنیم.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp :\n",
    "    def __init__(self,layer_sizes,input_size,activations):\n",
    "        if len(layer_sizes) == 1:\n",
    "            self.layers = [Layer(layer_sizes[0],input_size,1,activations[0])]\n",
    "            self.layers.append(Layer(1,layer_sizes[-1],0,activations[-2]))\n",
    "        elif len(layer_sizes) == 0:\n",
    "            self.layers = []\n",
    "            self.layers.append(Layer(1,input_size,0,activations[-1]))\n",
    "        else :\n",
    "            self.layers = [Layer(layer_sizes[0],input_size,layer_sizes[0],activations[0])]\n",
    "            for i in range(1,len(layer_sizes)-1):\n",
    "                self.layers.append(Layer(layer_sizes[i],layer_sizes[i-1],layer_sizes[i+1],activations[i]))\n",
    "            self.layers.append(Layer(layer_sizes[-1],layer_sizes[-2],1,activations[-2]))\n",
    "            self.layers.append(Layer(1,layer_sizes[-1],0,activations[-1]))\n",
    "        self.input = [0] * input_size\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.input\n",
    "        for i in range(len(self.layers)-1):\n",
    "            x = self.layers[i].forward(x)\n",
    "        x = self.layers[len(self.layers)-1].forward(x,True)\n",
    "        return x[0]\n",
    "       \n",
    "    def __call__(self):\n",
    "        self.forward()\n",
    "        pass\n",
    "    \n",
    "    def parameters(self):\n",
    "        weights = []\n",
    "        for i in range(len(self.layers)):\n",
    "            for j in range(len(self.layers[i].nodes)):\n",
    "                weights.append(self.layers[i].nodes[j].weights)\n",
    "        return weights\n",
    "    def set_input(self,x):\n",
    "        self.input = x\n",
    "\n",
    "    \n",
    "    def Backward(self,real_y,lr = 1):\n",
    "        if len(self.layers) == 1:\n",
    "            self.layers[0].backward(real_y,True)\n",
    "          \n",
    "        else:\n",
    "            self.layers[-1].backward(real_y,True)\n",
    "            self.layers[-2].set_nextLweights_and_deltas(self.layers[-1])\n",
    "            for i in range(len(self.layers)-2,0,-1):\n",
    "                self.layers[i].backward()\n",
    "                self.layers[i-1].set_nextLweights_and_deltas(self.layers[i])\n",
    "            self.layers[0].backward()\n",
    "            return self.getweights_prime() , self.getbias_prime()\n",
    "\n",
    "    def loss(self,real_y):\n",
    "        return (self.forward() - real_y)**2\n",
    "    \n",
    "    def set_weightsLi(self,layernum,weights):\n",
    "        for i in range(len(weights)):\n",
    "            self.layers[layernum].nodes[i].weights = weights[i]\n",
    "    def set_biasLi(self,layernum,bias):\n",
    "        for i in range(len(bias)):\n",
    "            self.layers[layernum].nodes[i].bias = bias[i]\n",
    "\n",
    "\n",
    "    def getweights_prime(self):\n",
    "        weights = []\n",
    "        for i in range(len(self.layers)):\n",
    "            weights.append(self.layers[i].getweights_prime())\n",
    "        return weights\n",
    "    \n",
    "    def getbias_prime(self):\n",
    "        bias = []\n",
    "        for i in range(len(self.layers)):\n",
    "            bias.append(self.layers[i].getbias_prime())\n",
    "        return bias\n",
    "    def set_weights_prime(self,weights):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].set_weights_prime(weights[i])\n",
    "    def set_bias_prime(self,bias):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].set_bias_prime(bias[i])\n",
    "    def update_weights_and_bias(self,lr):\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i].update_weights(lr)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "کلاس\n",
    "optimizer\n",
    "را میسازیم که این کلاس پارامتر هارا می گیردو خود \n",
    "model\n",
    "را\n",
    "و در قسمت\n",
    "step \n",
    "مقدار های وزن ها و بایاس ها را با استفاده از تابع\n",
    "update\n",
    "که در کلاس\n",
    "Mlp\n",
    "پیاده سازی شده است اپدیت می کند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self,lr,model: Mlp, parameters = None):\n",
    "    # Initialize the optimizer with the model's parameters and the learning rate\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.model = model\n",
    "    def zero_grad(self):\n",
    "    # Reset the gradients of all parameters to zero\n",
    "        self.model.weightsprime = [0] * len(self.model.weights)\n",
    "        self.model.biasprime = 0\n",
    "    def step(self):\n",
    "    # Update\n",
    "        self.model.update_weights_and_bias(self.lr)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "این تابع ها برای محاسبه میانگین وزن ها و بایاس هاست که از مشتق یک بچ میانگین میگیریم و به اندازه میانگین آن \n",
    "update\n",
    "می کنیم.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0.15000000000000002, 0.2, 0.3], [0.4, 0.5, 0.6]],\n",
       " [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def mean_w(weights):\n",
    "    sum_weights = weights[0].copy()\n",
    "    length = len(weights) \n",
    "\n",
    "\n",
    "    for weight in range(1,len(weights)):\n",
    "        for lw in range(len(weights[weight])):\n",
    "            for nw in range(len(weights[weight][lw])):\n",
    "                for w in range(len(weights[weight][lw][nw])):\n",
    "                    sum_weights[lw][nw][w]  = sum_weights[lw][nw][w] + weights[weight][lw][nw][w]\n",
    "    \n",
    "\n",
    "   \n",
    "    for lw in range(len(sum_weights)):\n",
    "        for nw in range(len(sum_weights[lw])):\n",
    "            for w in range(len(sum_weights[lw][nw])):\n",
    "                sum_weights[lw][nw][w] /= length\n",
    "    return sum_weights\n",
    "\n",
    "\n",
    "def mean_b(weights):\n",
    "    sum_weights = weights[0].copy()\n",
    "    length = len(weights)   \n",
    "    for weight in range(1,len(weights)):\n",
    "        for lw in range(len(weights[weight])):\n",
    "                for w in range(len(weights[weight][lw])):\n",
    "                    sum_weights[lw][w]  += (weights[weight][lw][w])\n",
    "\n",
    "   \n",
    "    for lw in range(len(sum_weights)):\n",
    "            for w in range(len(sum_weights[lw])):\n",
    "                sum_weights[lw][w] /= length\n",
    "    return sum_weights\n",
    "\n",
    "\n",
    "w = [[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]],[[[0.2, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]]]\n",
    "mean_w(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "در ابتدا یک دیتا ست میسازیم با استفاده از تابع \n",
    "make_blolbs\n",
    "که در دو کلاس است و 1000 \n",
    "نمونه دارد\n",
    "و سپس \n",
    "model\n",
    "را می سازیمکه دولایه است با 3و2 نورون\n",
    "و ساز input\n",
    "که تعداد \n",
    "فیچر هاست همان دو است در این مثال\n",
    "و ابجکت\n",
    "optimizer\n",
    "را میسازیم و \n",
    "model \n",
    "را به آن می دهیم \n",
    "و به ازای تعداد ایپاک ها که 1000 تاست فور میزنیم.\n",
    "\n",
    "weights_primes\n",
    "و\n",
    "bias_primes\n",
    "مشتق نسبت به وزن ها وبایاس هارا ذخیره می کنید که در نهیات میانگین بگیریم و در ادامه فور مینیم که به ازای یک ایپاک\n",
    "کل داده هارا ببینیم و مشتق نسبت وزن ها و بایاس های هر داده را ذخیره می کنیم \n",
    "ایتدا \n",
    "ورودی را به مذل می دهیم که همان \n",
    "x\n",
    "هر داده است\n",
    "و\n",
    "y_hat\n",
    "را که فوروارد یا کال کردن مدل است \n",
    "بدست می اورد\n",
    "و با \n",
    "Backward\n",
    "مشتق های پارامتر هارا بدست می اورین و به لست تمام مشتق ها و بایاس ها اضافه می کنیم.\n",
    "و در انتهای این \n",
    "for\n",
    "لاس برای این داده را محاسبه میکنمو با لاس های داده های قبلی در این بچ جمع می کنیم.\n",
    "سپس از مشتق های پارا متر ها با \n",
    "mean_weights_prime\n",
    "و\n",
    "mean_bias_prime\n",
    "میانگین می گیریم و با استفاده از این میانگین ها و\n",
    "step\n",
    "مدل را اپدیت می کنیم.\n",
    "که در نهایت اپذیت می کنیم همه پرا متر هارا و \n",
    "cost\n",
    "که میانگین لاست هارا با تقسیم مجموع به تعداد حساب میکنیم.\n",
    "و پرینت می کنیم که پیشرفت را ببینیم و در نهایت \n",
    "و در نهایت \n",
    "acuracy \n",
    "را با توجه به این که چند تا درست داده است و چند درصد درست داده چاپ می کنیم.\n",
    "و میبینم که \n",
    "cost \n",
    "کم میشود\n",
    "و\n",
    "accuracy \n",
    "هم زیاد می شود.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41037021630284454\n",
      "0.37911840059030044\n",
      "0.32755276385043375\n",
      "0.2686062110727972\n",
      "0.2210400752494141\n",
      "0.18266042774777036\n",
      "0.15329533761524353\n",
      "0.1319503833913877\n",
      "0.11651394817225694\n",
      "0.10496873710428842\n",
      "0.09589746396776695\n",
      "0.0884298153509802\n",
      "0.08205626142283255\n",
      "0.0764778812921298\n",
      "0.07151337073875698\n",
      "0.0670467672551857\n",
      "0.06299907327061642\n",
      "0.059312930290179394\n",
      "0.055944241851140025\n",
      "0.052857488597552515\n",
      "0.05002302749853243\n",
      "0.04741547832581982\n",
      "0.045012720307834125\n",
      "0.042795240015160524\n",
      "0.040745686579367835\n",
      "0.03884855239978999\n",
      "0.037089931850368074\n",
      "0.03545733001416526\n",
      "0.03393950479220757\n",
      "0.03252633238835286\n",
      "0.031208690106515034\n",
      "0.029978352718561644\n",
      "0.02882790002117737\n",
      "0.027750633987894704\n",
      "0.02674050437420191\n",
      "0.025792041891650914\n",
      "0.024900298216380127\n",
      "0.024060792188399904\n",
      "0.023269461618849695\n",
      "0.022522620168834644\n",
      "0.021816918803455054\n",
      "0.021149311362042608\n",
      "0.020517023821896838\n",
      "0.0199175268682868\n",
      "0.01934851141795368\n",
      "0.01880786677645067\n",
      "0.018293661141040356\n",
      "0.017804124190247107\n",
      "0.01733763152837447\n",
      "0.01689269077825445\n",
      "0.016467929138209804\n",
      "0.016062082239738373\n",
      "0.015673984160883638\n",
      "0.015302558466767362\n",
      "0.014946810163481193\n",
      "0.014605818464617979\n",
      "0.014278730281324137\n",
      "0.013964754357023828\n",
      "0.013663155977039184\n",
      "0.013373252191343476\n",
      "0.013094407495751997\n",
      "0.01282602992308924\n",
      "0.012567567501365399\n",
      "0.012318505040841447\n",
      "0.01207836121613258\n",
      "0.011846685913269097\n",
      "0.011623057814957724\n",
      "0.011407082200222008\n",
      "0.011198388937192846\n",
      "0.010996630650112775\n",
      "0.010801481043645172\n",
      "0.01061263336937375\n",
      "0.010429799020969827\n",
      "0.010252706245914343\n",
      "0.010081098962913978\n",
      "0.009914735675264052\n",
      "0.009753388471399468\n",
      "0.009596842104756033\n",
      "0.009444893145850255\n",
      "0.009297349200184166\n",
      "0.00915402818620883\n",
      "0.009014757668137143\n",
      "0.00887937423889767\n",
      "0.008747722948967784\n",
      "0.00861965677722703\n",
      "0.008495036140330049\n",
      "0.008373728437423383\n",
      "0.008255607627320187\n",
      "0.008140553835508931\n",
      "0.008028452988608374\n",
      "0.00791919647409276\n",
      "0.007812680823303855\n",
      "0.007708807415939301\n",
      "0.007607482204363856\n",
      "0.007508615456231534\n",
      "0.007412121514035539\n",
      "0.0073179185703191535\n",
      "0.00722592845738665\n",
      "0.007136076450449321\n",
      "0.007048291083228741\n",
      "0.006962503975119418\n",
      "0.006878649669084711\n",
      "0.006796665479526233\n",
      "0.00671649134942698\n",
      "0.006638069716123123\n",
      "0.006561345385110087\n",
      "0.006486265411333738\n",
      "0.006412778987460013\n",
      "0.00634083733865433\n",
      "0.00627039362343774\n",
      "0.006201402840218801\n",
      "0.006133821739129642\n",
      "0.006067608738822504\n",
      "0.0060027238479074055\n",
      "0.005939128590734942\n",
      "0.005876785937249387\n",
      "0.005815660236656637\n",
      "0.005755717154669624\n",
      "0.0056969236141104765\n",
      "0.005639247738663618\n",
      "0.005582658799588777\n",
      "0.005527127165215219\n",
      "0.005472624253051194\n",
      "0.005419122484353131\n",
      "0.005366595241010397\n",
      "0.005315016824609601\n",
      "0.005264362417552931\n",
      "0.005214608046111799\n",
      "0.005165730545305588\n",
      "0.005117707525502256\n",
      "0.005070517340643639\n",
      "0.005024139058005052\n",
      "0.0049785524294042125\n",
      "0.004933737863779537\n",
      "0.004889676401063358\n",
      "0.004846349687279736\n",
      "0.004803739950801008\n",
      "0.004761829979701094\n",
      "0.004720603100147563\n",
      "0.004680043155777643\n",
      "0.004640134488006686\n",
      "0.004600861917220722\n",
      "0.004562210724807526\n",
      "0.004524166635983183\n",
      "0.004486715803373711\n",
      "0.004449844791313641\n",
      "0.004413540560825546\n",
      "0.004377790455246438\n",
      "0.004342582186469318\n",
      "0.0043079038217693185\n",
      "0.004273743771185993\n",
      "0.004240090775434798\n",
      "0.0042069338943221775\n",
      "0.0041742624956401285\n",
      "0.0041420662445175145\n",
      "0.0041103350932063095\n",
      "0.004079059271282554\n",
      "0.0040482292762426065\n",
      "0.0040178358644761695\n",
      "0.003987870042598948\n",
      "0.003958323059128318\n",
      "0.003929186396486416\n",
      "0.003900451763315791\n",
      "0.003872111087093685\n",
      "0.0038441565070313245\n",
      "0.0038165803672458873\n",
      "0.0037893752101928195\n",
      "0.0037625337703471743\n",
      "0.0037360489681231516\n",
      "0.0037099139040213933\n",
      "0.003684121852994266\n",
      "0.0036586662590197857\n",
      "0.003633540729875237\n",
      "0.0036087390321021045\n",
      "0.0035842550861541335\n",
      "0.003560082961720919\n",
      "0.0035362168732197487\n",
      "0.0035126511754485987\n",
      "0.0034893803593937316\n",
      "0.0034663990481855643\n",
      "0.003443701993196747\n",
      "0.00342128407027657\n",
      "0.003399140276116507\n",
      "0.0033772657247412422\n",
      "0.0033556556441205307\n",
      "0.00333430537289686\n",
      "0.003313210357224493\n",
      "0.0032923661477154095\n",
      "0.0032717683964880754\n",
      "0.0032514128543149933\n",
      "0.003231295367865228\n",
      "0.0032114118770382384\n",
      "0.0031917584123855934\n",
      "0.0031723310926171644\n",
      "0.0031531261221886195\n",
      "0.003134139788967203\n",
      "0.0031153684619727984\n",
      "0.003096808589191508\n",
      "0.0030784566954590903\n",
      "0.0030603093804116455\n",
      "0.0030423633165010734\n",
      "0.00302461524707297\n",
      "0.0030070619845047083\n",
      "0.002989700408401436\n",
      "0.0029725274638480854\n",
      "0.0029555401597151534\n",
      "0.0029387355670165965\n",
      "0.0029221108173177685\n",
      "0.0029056631011917956\n",
      "0.0028893896667225786\n",
      "0.0028732878180528915\n",
      "0.0028573549139759525\n",
      "0.0028415883665689366\n",
      "0.0028259856398670535\n",
      "0.0028105442485767997\n",
      "0.002795261756826964\n",
      "0.0027801357769561665\n",
      "0.002765163968335775\n",
      "0.0027503440362268115\n",
      "0.002735673730669897\n",
      "0.0027211508454070645\n",
      "0.002706773216834341\n",
      "0.0026925387229841708\n",
      "0.002678445282536591\n",
      "0.00266449085385839\n",
      "0.002650673434069079\n",
      "0.0026369910581330925\n",
      "0.002623441797977168\n",
      "0.002610023761632149\n",
      "0.002596735092398497\n",
      "0.00258357396803465\n",
      "0.0025705385999676153\n",
      "0.0025576272325249665\n",
      "0.002544838142187694\n",
      "0.002532169636863207\n",
      "0.002519620055177803\n",
      "0.0025071877657881054\n",
      "0.0024948711667108227\n",
      "0.002482668684670231\n",
      "0.0024705787744629466\n",
      "0.002458599918339338\n",
      "0.0024467306254011803\n",
      "0.0024349694310149108\n",
      "0.002423314896240276\n",
      "0.0024117656072735506\n",
      "0.0024003201749052566\n",
      "0.0023889772339916824\n",
      "0.002377735442939916\n",
      "0.0023665934832059914\n",
      "0.002355550058805715\n",
      "0.002344603895837843\n",
      "0.0023337537420192026\n",
      "0.0023229983662315145\n",
      "0.0023123365580794107\n",
      "0.002301767127459484\n",
      "0.0022912889041399593\n",
      "0.0022809007373507445\n",
      "0.0022706014953834783\n",
      "0.002260390065201388\n",
      "0.0022502653520586383\n",
      "0.0022402262791288664\n",
      "0.0022302717871427184\n",
      "0.0022204008340341087\n",
      "0.0022106123945948948\n",
      "0.0022009054601378813\n",
      "0.002191279038167705\n",
      "0.0021817321520596574\n",
      "0.00217226384074598\n",
      "0.0021628731584095676\n",
      "0.0021535591741848395\n",
      "0.0021443209718655615\n",
      "0.002135157649619503\n",
      "0.0021260683197096676\n",
      "0.002117052108221954\n",
      "0.0021081081547990525\n",
      "0.002099235612380494\n",
      "0.002090433646948561\n",
      "0.0020817014372799957\n",
      "0.002073038174703367\n",
      "0.002064443062861816\n",
      "0.0020559153174812502\n",
      "0.0020474541661436294\n",
      "0.00203905884806537\n",
      "0.002030728613880651\n",
      "0.0020224627254295043\n",
      "0.002014260455550642\n",
      "0.0020061210878787724\n",
      "0.0019980439166463666\n",
      "0.001990028246489835\n",
      "0.0019820733922597937\n",
      "0.0019741786788355494\n",
      "0.0019663434409435436\n",
      "0.001958567022979721\n",
      "0.0019508487788356995\n",
      "0.0019431880717286681\n",
      "0.0019355842740349143\n",
      "0.0019280367671268676\n",
      "0.0019205449412136016\n",
      "0.0019131081951847153\n",
      "0.0019057259364574615\n",
      "0.001898397580827133\n",
      "0.0018911225523204906\n",
      "0.001883900283052343\n",
      "0.0018767302130850065\n",
      "0.0018696117902907157\n",
      "0.0018625444702168857\n",
      "0.0018555277159541038\n",
      "0.001848560998006825\n",
      "0.0018416437941667374\n",
      "0.0018347755893886673\n",
      "0.0018279558756690228\n",
      "0.0018211841519266794\n",
      "0.001814459923886252\n",
      "0.0018077827039637443\n",
      "0.0018011520111544605\n",
      "0.0017945673709231433\n",
      "0.00178802831509631\n",
      "0.0017815343817567358\n",
      "0.0017750851151399688\n",
      "0.0017686800655329557\n",
      "0.0017623187891745882\n",
      "0.0017560008481582203\n",
      "0.0017497258103361109\n",
      "0.0017434932492256866\n",
      "0.001737302743917624\n",
      "0.0017311538789857384\n",
      "0.0017250462443985648\n",
      "0.0017189794354326793\n",
      "0.0017129530525876366\n",
      "0.001706966701502561\n",
      "0.0017010199928743084\n",
      "0.0016951125423771804\n",
      "0.001689243970584165\n",
      "0.0016834139028896488\n",
      "0.0016776219694335937\n",
      "0.0016718678050271075\n",
      "0.0016661510490794524\n",
      "0.0016604713455263568\n",
      "0.0016548283427596851\n",
      "0.0016492216935584242\n",
      "0.0016436510550209157\n",
      "0.0016381160884983137\n",
      "0.0016326164595293557\n",
      "0.0016271518377762068\n",
      "0.001621721896961549\n",
      "0.0016163263148068006\n",
      "0.001610964772971432\n",
      "0.0016056369569934193\n",
      "0.0016003425562307108\n",
      "0.0015950812638038162\n",
      "0.0015898527765393512\n",
      "0.0015846567949146615\n",
      "0.0015794930230033637\n",
      "0.0015743611684219136\n",
      "0.0015692609422770828\n",
      "0.001564192059114364\n",
      "0.001559154236867321\n",
      "0.0015541471968077575\n",
      "0.0015491706634968392\n",
      "0.0015442243647370195\n",
      "0.0015393080315248026\n",
      "0.001534421398004342\n",
      "0.0015295642014218531\n",
      "0.0015247361820807579\n",
      "0.0015199370832976805\n",
      "0.00151516665135913\n",
      "0.0015104246354789705\n",
      "0.0015057107877565715\n",
      "0.0015010248631357197\n",
      "0.0014963666193642084\n",
      "0.0014917358169540774\n",
      "0.0014871322191425903\n",
      "0.0014825555918538047\n",
      "0.0014780057036608305\n",
      "0.0014734823257487029\n",
      "0.0014689852318778762\n",
      "0.001464514198348309\n",
      "0.0014600690039641994\n",
      "0.0014556494299992281\n",
      "0.0014512552601624474\n",
      "0.00144688628056468\n",
      "0.001442542279685489\n",
      "0.0014382230483407086\n",
      "0.001433928379650466\n",
      "0.0014296580690077506\n",
      "0.0014254119140474913\n",
      "0.0014211897146161369\n",
      "0.001416991272741706\n",
      "0.0014128163926043515\n",
      "0.001408664880507385\n",
      "0.0014045365448487608\n",
      "0.0014004311960930116\n",
      "0.0013963486467436597\n",
      "0.0013922887113160362\n",
      "0.0013882512063105455\n",
      "0.0013842359501863555\n",
      "0.0013802427633354964\n",
      "0.001376271468057374\n",
      "0.0013723218885336712\n",
      "0.0013683938508036698\n",
      "0.001364487182739919\n",
      "0.0013606017140243272\n",
      "0.0013567372761245761\n",
      "0.0013528937022709575\n",
      "0.0013490708274335147\n",
      "0.0013452684882995717\n",
      "0.0013414865232515955\n",
      "0.0013377247723454055\n",
      "0.0013339830772887004\n",
      "0.0013302612814199378\n",
      "0.0013265592296875127\n",
      "0.0013228767686292685\n",
      "0.0013192137463523077\n",
      "0.001315570012513124\n",
      "0.0013119454182980066\n",
      "0.0013083398164037672\n",
      "0.0013047530610187596\n",
      "0.001301185007804134\n",
      "0.0012976355138754477\n",
      "0.00129410443778448\n",
      "0.0012905916395013638\n",
      "0.0012870969803969645\n",
      "0.0012836203232255135\n",
      "0.0012801615321075175\n",
      "0.0012767204725128958\n",
      "0.0012732970112443863\n",
      "0.0012698910164211917\n",
      "0.001266502357462843\n",
      "0.0012631309050733282\n",
      "0.0012597765312254397\n",
      "0.0012564391091453425\n",
      "0.0012531185132973724\n",
      "0.0012498146193690555\n",
      "0.0012465273042563433\n",
      "0.0012432564460490565\n",
      "0.0012400019240165411\n",
      "0.0012367636185935298\n",
      "0.0012335414113661964\n",
      "0.0012303351850584297\n",
      "0.0012271448235182804\n",
      "0.0012239702117046193\n",
      "0.0012208112356739639\n",
      "0.0012176677825675058\n",
      "0.0012145397405983252\n",
      "0.00121142699903876\n",
      "0.0012083294482079937\n",
      "0.0012052469794597658\n",
      "0.0012021794851703052\n",
      "0.0011991268587263806\n",
      "0.0011960889945135749\n",
      "0.0011930657879046615\n",
      "0.001190057135248191\n",
      "0.0011870629338572023\n",
      "0.0011840830819981115\n",
      "0.001181117478879736\n",
      "0.0011781660246424888\n",
      "0.0011752286203476922\n",
      "0.0011723051679670695\n",
      "0.0011693955703723589\n",
      "0.0011664997313250716\n",
      "0.001163617555466396\n",
      "0.001160748948307237\n",
      "0.0011578938162183743\n",
      "0.001155052066420777\n",
      "0.0011522236069760401\n",
      "0.001149408346776927\n",
      "0.0011466061955380871\n",
      "0.0011438170637868446\n",
      "0.0011410408628541445\n",
      "0.0011382775048656\n",
      "0.0011355269027326811\n",
      "0.001132788970143992\n",
      "0.0011300636215566897\n",
      "0.0011273507721879872\n",
      "0.001124650338006801\n",
      "0.0011219622357254819\n",
      "0.0011192863827916746\n",
      "0.0011166226973802597\n",
      "0.001113971098385422\n",
      "0.001111331505412815\n",
      "0.0011087038387718304\n",
      "0.0011060880194679568\n",
      "0.0011034839691952372\n",
      "0.0011008916103288546\n",
      "0.001098310865917771\n",
      "0.0010957416596774818\n",
      "0.0010931839159828715\n",
      "0.0010906375598611347\n",
      "0.0010881025169848174\n",
      "0.0010855787136649201\n",
      "0.0010830660768441178\n",
      "0.0010805645340900257\n",
      "0.001078074013588601\n",
      "0.0010755944441375792\n",
      "0.0010731257551400272\n",
      "0.0010706678765979663\n",
      "0.0010682207391060636\n",
      "0.001065784273845418\n",
      "0.001063358412577431\n",
      "0.0010609430876377265\n",
      "0.0010585382319301714\n",
      "0.001056143778920953\n",
      "0.0010537596626327553\n",
      "0.0010513858176389639\n",
      "0.0010490221790579984\n",
      "0.0010466686825476605\n",
      "0.0010443252642995914\n",
      "0.0010419918610337785\n",
      "0.0010396684099931357\n",
      "0.0010373548489381362\n",
      "0.0010350511161415432\n",
      "0.0010327571503831608\n",
      "0.001030472890944692\n",
      "0.0010281982776046217\n",
      "0.0010259332506331962\n",
      "0.0010236777507874359\n",
      "0.001021431719306212\n",
      "0.0010191950979054106\n",
      "0.0010169678287731102\n",
      "0.00101474985456485\n",
      "0.0010125411183989552\n",
      "0.0010103415638518921\n",
      "0.0010081511349536942\n",
      "0.001005969776183461\n",
      "0.0010037974324648758\n",
      "0.0010016340491617974\n",
      "0.0009994795720739053\n",
      "0.000997333947432385\n",
      "0.000995197121895668\n",
      "0.0009930690425452291\n",
      "0.0009909496568814324\n",
      "0.0009888389128194096\n",
      "0.000986736758685006\n",
      "0.000984643143210765\n",
      "0.0009825580155319613\n",
      "0.000980481325182679\n",
      "0.0009784130220919364\n",
      "0.0009763530565798515\n",
      "0.0009743013793538633\n",
      "0.0009722579415049783\n",
      "0.0009702226945040843\n",
      "0.0009681955901982832\n",
      "0.0009661765808072776\n",
      "0.0009641656189197986\n",
      "0.0009621626574900742\n",
      "0.0009601676498343375\n",
      "0.0009581805496273663\n",
      "0.00095620131089908\n",
      "0.0009542298880311609\n",
      "0.0009522662357537139\n",
      "0.0009503103091419766\n",
      "0.0009483620636130451\n",
      "0.0009464214549226686\n",
      "0.0009444884391620376\n",
      "0.0009425629727546502\n",
      "0.0009406450124531875\n",
      "0.0009387345153364292\n",
      "0.0009368314388062084\n",
      "0.000934935740584401\n",
      "0.0009330473787099386\n",
      "0.0009311663115358673\n",
      "0.0009292924977264241\n",
      "0.000927425896254169\n",
      "0.0009255664663971197\n",
      "0.0009237141677359418\n",
      "0.0009218689601511557\n",
      "0.0009200308038203863\n",
      "0.0009181996592156232\n",
      "0.0009163754871005399\n",
      "0.0009145582485278075\n",
      "0.0009127479048364745\n",
      "0.0009109444176493413\n",
      "0.0009091477488703857\n",
      "0.0009073578606822158\n",
      "0.0009055747155435278\n",
      "0.0009037982761866188\n",
      "0.0009020285056149179\n",
      "0.0009002653671005323\n",
      "0.0008985088241818271\n",
      "0.0008967588406610477\n",
      "0.0008950153806019368\n",
      "0.0008932784083273992\n",
      "0.0008915478884171824\n",
      "0.0008898237857055924\n",
      "0.000888106065279215\n",
      "0.0008863946924746798\n",
      "0.0008846896328764312\n",
      "0.00088299085231455\n",
      "0.0008812983168625603\n",
      "0.0008796119928352916\n",
      "0.0008779318467867476\n",
      "0.0008762578455079994\n",
      "0.0008745899560251073\n",
      "0.0008729281455970566\n",
      "0.0008712723817137149\n",
      "0.0008696226320938177\n",
      "0.0008679788646829767\n",
      "0.0008663410476516904\n",
      "0.000864709149393391\n",
      "0.0008630831385225261\n",
      "0.000861462983872616\n",
      "0.0008598486544943801\n",
      "0.000858240119653846\n",
      "0.0008566373488305029\n",
      "0.0008550403117154586\n",
      "0.0008534489782096229\n",
      "0.0008518633184218978\n",
      "0.0008502833026674149\n",
      "0.0008487089014657541\n",
      "0.0008471400855392066\n",
      "0.0008455768258110428\n",
      "0.0008440190934038092\n",
      "0.0008424668596376239\n",
      "0.0008409200960285083\n",
      "0.0008393787742867336\n",
      "0.0008378428663151622\n",
      "0.0008363123442076432\n",
      "0.0008347871802473844\n",
      "0.0008332673469053703\n",
      "0.0008317528168387816\n",
      "0.0008302435628894357\n",
      "0.0008287395580822371\n",
      "0.0008272407756236554\n",
      "0.0008257471889001962\n",
      "0.0008242587714769192\n",
      "0.0008227754970959408\n",
      "0.0008212973396749624\n",
      "0.000819824273305831\n",
      "0.0008183562722530747\n",
      "0.0008168933109524969\n",
      "0.0008154353640097493\n",
      "0.0008139824061989357\n",
      "0.0008125344124612357\n",
      "0.0008110913579035197\n",
      "0.0008096532177970002\n",
      "0.0008082199675758848\n",
      "0.0008067915828360399\n",
      "0.0008053680393336752\n",
      "0.0008039493129840422\n",
      "0.0008025353798601281\n",
      "0.000801126216191393\n",
      "0.0007997217983624821\n",
      "0.0007983221029119885\n",
      "0.0007969271065311909\n",
      "0.0007955367860628359\n",
      "0.000794151118499914\n",
      "0.0007927700809844417\n",
      "0.0007913936508062825\n",
      "0.0007900218054019458\n",
      "0.0007886545223534183\n",
      "0.0007872917793870075\n",
      "0.000785933554372174\n",
      "0.000784579825320412\n",
      "0.0007832305703841003\n",
      "0.0007818857678553947\n",
      "0.0007805453961651156\n",
      "0.0007792094338816511\n",
      "0.0007778778597098642\n",
      "0.0007765506524900212\n",
      "0.0007752277911967269\n",
      "0.0007739092549378544\n",
      "0.0007725950229535148\n",
      "0.0007712850746150078\n",
      "0.0007699793894237964\n",
      "0.0007686779470104897\n",
      "0.0007673807271338344\n",
      "0.0007660877096797182\n",
      "0.0007647988746601695\n",
      "0.0007635142022123943\n",
      "0.0007622336725977815\n",
      "0.0007609572662009621\n",
      "0.0007596849635288448\n",
      "0.0007584167452096678\n",
      "0.0007571525919920681\n",
      "0.0007558924847441569\n",
      "0.0007546364044525911\n",
      "0.0007533843322216685\n",
      "0.0007521362492724279\n",
      "0.0007508921369417449\n",
      "0.0007496519766814609\n",
      "0.0007484157500574891\n",
      "0.000747183438748958\n",
      "0.0007459550245473403\n",
      "0.0007447304893556057\n",
      "0.000743509815187368\n",
      "0.0007422929841660518\n",
      "0.0007410799785240601\n",
      "0.0007398707806019457\n",
      "0.0007386653728476015\n",
      "0.0007374637378154524\n",
      "0.0007362658581656422\n",
      "0.0007350717166632576\n",
      "0.0007338812961775214\n",
      "0.0007326945796810299\n",
      "0.0007315115502489657\n",
      "0.0007303321910583411\n",
      "0.0007291564853872302\n",
      "0.0007279844166140256\n",
      "0.000726815968216685\n",
      "0.0007256511237719929\n",
      "0.0007244898669548294\n",
      "0.0007233321815374462\n",
      "0.0007221780513887395\n",
      "0.0007210274604735443\n",
      "0.00071988039285192\n",
      "0.0007187368326784529\n",
      "0.000717596764201564\n",
      "0.0007164601717628147\n",
      "0.0007153270397962256\n",
      "0.0007141973528276016\n",
      "0.0007130710954738581\n",
      "0.0007119482524423622\n",
      "0.0007108288085302597\n",
      "0.0007097127486238377\n",
      "0.0007086000576978682\n",
      "0.0007074907208149649\n",
      "0.0007063847231249484\n",
      "0.000705282049864219\n",
      "0.0007041826863551238\n",
      "0.0007030866180053385\n",
      "0.0007019938303072578\n",
      "0.0007009043088373758\n",
      "0.0006998180392556922\n",
      "0.0006987350073051051\n",
      "0.0006976551988108187\n",
      "0.0006965785996797569\n",
      "0.0006955051958999769\n",
      "0.0006944349735400878\n",
      "0.0006933679187486858\n",
      "0.0006923040177537721\n",
      "0.0006912432568621927\n",
      "0.0006901856224590901\n",
      "0.0006891311010073306\n",
      "0.0006880796790469645\n",
      "0.0006870313431946773\n",
      "0.0006859860801432559\n",
      "0.0006849438766610344\n",
      "0.0006839047195913873\n",
      "0.0006828685958521792\n",
      "0.0006818354924352522\n",
      "0.0006808053964059114\n",
      "0.0006797782949024003\n",
      "0.0006787541751353979\n",
      "0.0006777330243875078\n",
      "0.0006767148300127676\n",
      "0.0006756995794361362\n",
      "0.0006746872601530112\n",
      "0.000673677859728734\n",
      "0.000672671365798111\n",
      "0.0006716677660649271\n",
      "0.0006706670483014699\n",
      "0.0006696692003480572\n",
      "0.0006686742101125687\n",
      "0.0006676820655699767\n",
      "0.0006666927547618903\n",
      "0.0006657062657960929\n",
      "0.0006647225868460896\n",
      "0.0006637417061506562\n",
      "0.0006627636120133972\n",
      "0.0006617882928022947\n",
      "0.0006608157369492745\n",
      "0.0006598459329497705\n",
      "0.0006588788693622901\n",
      "0.0006579145348079856\n",
      "0.0006569529179702311\n",
      "0.0006559940075941992\n",
      "0.0006550377924864399\n",
      "0.000654084261514474\n",
      "0.0006531334036063682\n",
      "0.0006521852077503406\n",
      "0.0006512396629943473\n",
      "0.0006502967584456796\n",
      "0.0006493564832705721\n",
      "0.0006484188266938025\n",
      "0.0006474837779983004\n",
      "0.0006465513265247567\n",
      "0.0006456214616712411\n",
      "0.0006446941728928133\n",
      "0.0006437694497011525\n",
      "0.0006428472816641715\n",
      "0.0006419276584056445\n",
      "0.0006410105696048428\n",
      "0.0006400960049961591\n",
      "0.0006391839543687433\n",
      "0.0006382744075661483\n",
      "0.0006373673544859587\n",
      "0.0006364627850794427\n",
      "0.0006355606893511979\n",
      "0.0006346610573587981\n",
      "0.0006337638792124392\n",
      "0.0006328691450746117\n",
      "0.0006319768451597375\n",
      "0.0006310869697338424\n",
      "0.0006301995091142146\n",
      "0.0006293144536690724\n",
      "0.0006284317938172265\n",
      "0.000627551520027758\n",
      "0.0006266736228196857\n",
      "0.0006257980927616455\n",
      "0.0006249249204715627\n",
      "0.0006240540966163404\n",
      "0.000623185611911537\n",
      "0.0006223194571210554\n",
      "0.0006214556230568226\n",
      "0.0006205941005784929\n",
      "0.0006197348805931325\n",
      "0.0006188779540549126\n",
      "0.0006180233119648116\n",
      "0.0006171709453703176\n",
      "0.0006163208453651202\n",
      "0.0006154730030888223\n",
      "0.0006146274097266464\n",
      "0.0006137840565091417\n",
      "0.0006129429347118964\n",
      "0.0006121040356552448\n",
      "0.0006112673507039977\n",
      "0.0006104328712671384\n",
      "0.0006096005887975615\n",
      "0.0006087704947917868\n",
      "0.0006079425807896807\n",
      "0.0006071168383741896\n",
      "0.0006062932591710568\n",
      "0.0006054718348485666\n",
      "0.0006046525571172624\n",
      "0.0006038354177296931\n",
      "0.0006030204084801403\n",
      "0.000602207521204362\n",
      "0.0006013967477793276\n",
      "0.0006005880801229662\n",
      "0.0005997815101939074\n",
      "0.000598977029991222\n",
      "0.0005981746315541785\n",
      "0.0005973743069619903\n",
      "0.0005965760483335639\n",
      "0.0005957798478272521\n",
      "0.0005949856976406147\n",
      "0.0005941935900101714\n",
      "0.0005934035172111581\n",
      "0.0005926154715572959\n",
      "0.0005918294454005425\n",
      "0.0005910454311308656\n",
      "0.0005902634211760041\n",
      "0.0005894834080012356\n",
      "0.0005887053841091465\n",
      "0.0005879293420394043\n",
      "0.0005871552743685272\n",
      "0.0005863831737096636\n",
      "0.0005856130327123574\n",
      "0.0005848448440623403\n",
      "0.000584078600481299\n",
      "0.0005833142947266626\n",
      "0.0005825519195913798\n",
      "0.00058179146790371\n",
      "0.0005810329325269975\n",
      "0.0005802763063594749\n",
      "0.0005795215823340354\n",
      "0.0005787687534180328\n",
      "0.0005780178126130697\n",
      "0.0005772687529547927\n",
      "0.000576521567512686\n",
      "0.0005757762493898639\n",
      "0.0005750327917228753\n",
      "0.0005742911876814978\n",
      "0.000573551430468541\n",
      "0.0005728135133196445\n",
      "0.0005720774295030884\n",
      "0.0005713431723195873\n",
      "0.0005706107351021089\n",
      "0.0005698801112156702\n",
      "0.0005691512940571557\n",
      "0.0005684242770551209\n",
      "0.0005676990536696051\n",
      "0.0005669756173919513\n",
      "0.0005662539617446111\n",
      "0.000565534080280961\n",
      "0.0005648159665851279\n",
      "0.0005640996142718002\n",
      "0.0005633850169860476\n",
      "0.0005626721684031442\n",
      "0.0005619610622283878\n",
      "0.000561251692196926\n",
      "0.0005605440520735802\n",
      "0.0005598381356526691\n",
      "0.0005591339367578359\n",
      "0.000558431449241881\n",
      "0.0005577306669865857\n",
      "0.000557031583902546\n",
      "0.0005563341939290036\n",
      "0.0005556384910336755\n",
      "0.0005549444692125957\n",
      "0.0005542521224899446\n",
      "0.0005535614449178836\n",
      "0.0005528724305763993\n",
      "0.0005521850735731358\n",
      "0.0005514993680432417\n",
      "0.0005508153081491993\n",
      "0.0005501328880806797\n",
      "0.000549452102054376\n",
      "0.0005487729443138532\n",
      "0.0005480954091293931\n",
      "0.0005474194907978336\n",
      "0.0005467451836424256\n",
      "0.0005460724820126754\n",
      "0.000545401380284196\n",
      "0.0005447318728585567\n",
      "0.0005440639541631384\n",
      "0.0005433976186509778\n",
      "0.0005427328608006264\n",
      "0.0005420696751160107\n",
      "0.0005414080561262718\n",
      "0.000540747998385641\n",
      "0.0005400894964732796\n",
      "0.0005394325449931515\n",
      "0.0005387771385738726\n",
      "0.0005381232718685762\n",
      "0.0005374709395547727\n",
      "0.0005368201363342124\n",
      "0.0005361708569327473\n",
      "0.0005355230961001966\n",
      "0.0005348768486102124\n",
      "0.0005342321092601429\n",
      "0.0005335888728708985\n",
      "0.0005329471342868277\n",
      "0.0005323068883755722\n",
      "0.0005316681300279482\n",
      "0.0005310308541578084\n",
      "0.000530395055701921\n",
      "0.0005297607296198357\n",
      "0.0005291278708937563\n",
      "0.0005284964745284201\n",
      "0.0005278665355509648\n",
      "0.0005272380490108128\n",
      "0.0005266110099795378\n",
      "0.0005259854135507487\n",
      "0.0005253612548399678\n",
      "0.0005247385289845012\n",
      "0.0005241172311433299\n",
      "0.0005234973564969806\n",
      "0.0005228789002474105\n",
      "0.0005222618576178911\n",
      "0.0005216462238528883\n",
      "0.0005210319942179429\n",
      "0.0005204191639995626\n",
      "0.0005198077285051\n",
      "0.0005191976830626405\n",
      "0.0005185890230208892\n",
      "0.0005179817437490576\n",
      "0.0005173758406367514\n",
      "0.0005167713090938609\n",
      "0.0005161681445504438\n",
      "0.0005155663424566257\n",
      "0.0005149658982824839\n",
      "0.0005143668075179387\n",
      "0.000513769065672648\n",
      "0.0005131726682758983\n",
      "0.000512577610876504\n",
      "0.0005119838890426913\n",
      "0.0005113914983619999\n",
      "0.0005108004344411824\n",
      "0.0005102106929060919\n",
      "0.0005096222694015816\n",
      "0.0005090351595914066\n",
      "0.000508449359158119\n",
      "0.0005078648638029678\n",
      "0.000507281669245795\n",
      "0.0005066997712249414\n",
      "0.0005061191654971432\n",
      "0.0005055398478374389\n",
      "0.0005049618140390625\n",
      "0.0005043850599133542\n",
      "0.000503809581289662\n",
      "0.0005032353740152413\n",
      "0.000502662433955167\n",
      "0.000502090756992233\n",
      "0.0005015203390268574\n",
      "0.0005009511759769957\n",
      "0.0005003832637780406\n",
      "0.0004998165983827308\n",
      "0.0004992511757610661\n",
      "0.0004986869919002043\n",
      "0.0004981240428043813\n",
      "0.0004975623244948159\n",
      "0.00049700183300962\n",
      "0.0004964425644037134\n",
      "0.0004958845147487283\n",
      "0.0004953276801329297\n",
      "0.0004947720566611233\n",
      "0.0004942176404545686\n",
      "0.0004936644276508954\n",
      "0.0004931124144040177\n",
      "0.0004925615968840426\n",
      "0.0004920119712771962\n",
      "0.0004914635337857295\n",
      "0.0004909162806278421\n",
      "0.0004903702080375958\n",
      "0.0004898253122648318\n",
      "0.0004892815895750894\n",
      "0.0004887390362495255\n",
      "0.000488197648584832\n",
      "0.0004876574228931562\n",
      "0.9730361977503674\n",
      "0.9805426566751404\n",
      "0.021354305810547508\n",
      "0.9810691337223176\n",
      "0.9811668331647353\n",
      "0.9795892053337955\n",
      "0.02148473013227499\n",
      "0.979686572061726\n",
      "0.9761665216512012\n",
      "0.021922122077650357\n",
      "0.9780930752877616\n",
      "0.9780815458766832\n",
      "0.02182551585964224\n",
      "0.9812200631250515\n",
      "0.02210528713916375\n",
      "0.9775415137965696\n",
      "0.9813446935670203\n",
      "0.02171447683178227\n",
      "0.021615549743886887\n",
      "0.021300270182990317\n",
      "0.02110995240952846\n",
      "0.9817295980078572\n",
      "0.02172308931485555\n",
      "0.9746366868475472\n",
      "0.9819686864096557\n",
      "0.9811475393810578\n",
      "0.9818578598781275\n",
      "0.9699932987287365\n",
      "0.022565755444006328\n",
      "0.9792492259538494\n",
      "0.9808696262336047\n",
      "0.02130172988398436\n",
      "0.9754207466475477\n",
      "0.021345638844037274\n",
      "0.02170033578675053\n",
      "0.021460416064552318\n",
      "0.021325536594871956\n",
      "0.021195082702235968\n",
      "0.02151908977094624\n",
      "0.02267040373640817\n",
      "0.9743516174964275\n",
      "0.02164582493909133\n",
      "0.9812821501539185\n",
      "0.9758101441451409\n",
      "0.9776093095049263\n",
      "0.02188084395538721\n",
      "0.9808595530655824\n",
      "0.9781948701595138\n",
      "0.021400152755834047\n",
      "0.021509619675989827\n",
      "0.9688358024600771\n",
      "0.9717694409961963\n",
      "0.9780787532330126\n",
      "0.9808117954447612\n",
      "0.021130407868043516\n",
      "0.976577382623476\n",
      "0.9732832259032292\n",
      "0.9820747696592149\n",
      "0.9800743093821795\n",
      "0.021299716656266163\n",
      "0.02128140254494938\n",
      "0.02148960819685496\n",
      "0.9760716412243154\n",
      "0.9818347406364607\n",
      "0.9756673070036757\n",
      "0.021456109838336828\n",
      "0.021863516441864357\n",
      "0.022126809432052096\n",
      "0.02128865485630504\n",
      "0.021171951541973454\n",
      "0.021192291147548645\n",
      "0.022543215390091784\n",
      "0.9639125559673357\n",
      "0.021130547794985355\n",
      "0.021214033165028953\n",
      "0.9809453346050533\n",
      "0.9796465040806258\n",
      "0.9755657111416506\n",
      "0.021302393919389935\n",
      "0.9680711576012676\n",
      "0.021212151162446586\n",
      "0.021156717789223567\n",
      "0.9815935128706597\n",
      "0.02135728002525962\n",
      "0.9802446786733515\n",
      "0.9753776846046912\n",
      "0.021331824252641667\n",
      "0.021223392227794236\n",
      "0.979349198952033\n",
      "0.9779537224050397\n",
      "0.021155804550683093\n",
      "0.022257621488922217\n",
      "0.9805676565912609\n",
      "0.021238895016588585\n",
      "0.9734197751514627\n",
      "0.021555581501115553\n",
      "0.9787607445586722\n",
      "0.9820063575704355\n",
      "0.9696108598174568\n",
      "0.974033899628694\n",
      "0.9783610762973929\n",
      "0.979895101382354\n",
      "0.021454568123614074\n",
      "0.02256043750028173\n",
      "0.9779879895337035\n",
      "0.9820156774542388\n",
      "0.021107878287058682\n",
      "0.9804966339929286\n",
      "0.9802316398103326\n",
      "0.9769904089219547\n",
      "0.021101378635474524\n",
      "0.9740895083475215\n",
      "0.9754592212141654\n",
      "0.02229454896895082\n",
      "0.9766806773332952\n",
      "0.02194419718952659\n",
      "0.021268909154164236\n",
      "0.981286308058646\n",
      "0.02124805866310081\n",
      "0.9707689736847237\n",
      "0.9812250285676488\n",
      "0.9777625600009501\n",
      "0.979762693428133\n",
      "0.9790073053904658\n",
      "0.021323663294145628\n",
      "0.9806587158763855\n",
      "0.9800094425888586\n",
      "0.9771035661577331\n",
      "0.9782565739259035\n",
      "0.9795871302823719\n",
      "0.021462421583853017\n",
      "0.02120114878817512\n",
      "0.021238505142150282\n",
      "0.980883474744238\n",
      "0.021909850542229593\n",
      "0.981300656316768\n",
      "0.021581833680341403\n",
      "0.9772377012809546\n",
      "0.022040361210386928\n",
      "0.9781664049672503\n",
      "0.021811633814707666\n",
      "0.02173582705951438\n",
      "0.9766145470106858\n",
      "0.9791768300554147\n",
      "0.02131112455087411\n",
      "0.9818684914311039\n",
      "0.9790969540717833\n",
      "0.02145318140490713\n",
      "0.021394737783535324\n",
      "0.021321987373115414\n",
      "0.02138502790301385\n",
      "0.9732015472745584\n",
      "0.9806702763352008\n",
      "0.9789481913694654\n",
      "0.981488904715859\n",
      "0.0211832492352063\n",
      "0.9793000947940428\n",
      "0.9809672666367195\n",
      "0.021337072110134164\n",
      "0.9788955329680636\n",
      "0.9775244200157159\n",
      "0.9800572313409817\n",
      "0.9739887972488992\n",
      "0.02179044519103368\n",
      "0.021698230087355736\n",
      "0.9815592012929191\n",
      "0.021251378498074538\n",
      "0.022713090364504376\n",
      "0.021837832928074022\n",
      "0.021473902376423864\n",
      "0.9774405647455128\n",
      "0.021449528728412947\n",
      "0.021743068548120026\n",
      "0.021758274188006237\n",
      "0.021152026877576436\n",
      "0.02154426185581844\n",
      "0.9684907157596022\n",
      "0.0211078009637755\n",
      "0.021271489059842004\n",
      "0.021436465638935535\n",
      "0.9796094198641434\n",
      "0.980430718331624\n",
      "0.9793274759247725\n",
      "0.021378804807421076\n",
      "0.02151374179563788\n",
      "0.021140330703061583\n",
      "0.981203515992253\n",
      "0.9817516818788411\n",
      "0.9769824695176512\n",
      "0.021351103255957855\n",
      "0.02137502284995764\n",
      "0.9812650692959229\n",
      "0.021382313702106723\n",
      "0.9700393890433722\n",
      "0.02129520550833058\n",
      "0.02142198277266267\n",
      "0.021571347500022812\n",
      "0.021325653381721568\n",
      "0.9696149212265571\n",
      "0.02147033642612672\n",
      "0.021611235457388297\n",
      "0.9798578248664223\n",
      "0.9792290052190539\n",
      "0.021663501346120474\n",
      "0.021166870647632657\n",
      "0.9805331184523391\n",
      "0.9796967827874037\n",
      "0.021095048624232438\n",
      "0.021802735339757955\n",
      "0.02133622860573792\n",
      "0.9798713873416858\n",
      "0.9799940045413232\n",
      "0.9785192646043712\n",
      "0.02129982622360638\n",
      "0.021217361884022557\n",
      "0.9776456837043109\n",
      "0.9734250221037962\n",
      "0.02231497233457685\n",
      "0.02186136901509831\n",
      "0.021668005670623387\n",
      "0.981408633513776\n",
      "0.9781081606010242\n",
      "0.021327368482343782\n",
      "0.02112586469691211\n",
      "0.9720264580775563\n",
      "0.021132197975153703\n",
      "0.9595047570415262\n",
      "0.9812640162888\n",
      "0.9767284677978638\n",
      "0.974366411189003\n",
      "0.9788976684271216\n",
      "0.974597323369881\n",
      "0.9796752490669599\n",
      "0.02185165241725745\n",
      "0.9780826529843379\n",
      "0.021195985059948495\n",
      "0.021931356455738866\n",
      "0.021465223812209198\n",
      "0.9783297254039054\n",
      "0.02126339595459138\n",
      "0.9797643728969828\n",
      "0.02136985574119465\n",
      "0.021552341327075997\n",
      "0.9783021651595633\n",
      "0.9799824517050599\n",
      "0.9785623183429649\n",
      "0.021556275144583286\n",
      "0.021199590223308144\n",
      "0.021307041683352394\n",
      "0.9819132388324325\n",
      "0.9811346187197201\n",
      "0.9814340804428471\n",
      "0.02265037357584\n",
      "0.02297548884719045\n",
      "0.021302901660290984\n",
      "0.021541871313802752\n",
      "0.021631051129252368\n",
      "0.9801546172423311\n",
      "0.9735232452904773\n",
      "0.021511491492752173\n",
      "0.9808862918150545\n",
      "0.9778635027804243\n",
      "0.021750140427044096\n",
      "0.9729144610443737\n",
      "0.021449246672242222\n",
      "0.021186205709158853\n",
      "0.02116707232322053\n",
      "0.02122026596091137\n",
      "0.02113615220781863\n",
      "0.9799930165768038\n",
      "0.9782216746519041\n",
      "0.9785618481870657\n",
      "0.021291092145877603\n",
      "0.980229101168478\n",
      "0.021262494476358567\n",
      "0.9755607630082871\n",
      "0.021757424888069345\n",
      "0.021406694189913526\n",
      "0.021571562999717422\n",
      "0.021465149017612872\n",
      "0.021262005436617163\n",
      "0.021134549124499073\n",
      "0.9759930232203946\n",
      "0.9784932069995035\n",
      "0.021644629121864385\n",
      "0.980005561596339\n",
      "0.021371723456153832\n",
      "0.021265580563200027\n",
      "0.9813905211270986\n",
      "0.9816529984410173\n",
      "0.9811610728223515\n",
      "0.9817554913548453\n",
      "0.021330427842718415\n",
      "0.021223490043076616\n",
      "0.9755855531387432\n",
      "0.021791021467730473\n",
      "0.9779054305279758\n",
      "0.021915021932777782\n",
      "0.022010034987189325\n",
      "0.981242082438479\n",
      "0.02145340820932573\n",
      "0.021260478772309303\n",
      "0.9798093851437574\n",
      "0.021254303610994764\n",
      "0.021259742388634838\n",
      "0.9818485094417023\n",
      "0.9749198280570156\n",
      "0.021829135480300712\n",
      "0.9806782829754531\n",
      "0.021075545854942177\n",
      "0.9717622307001919\n",
      "0.02126915645288049\n",
      "0.0213154545534556\n",
      "0.9799919577339964\n",
      "0.9815317842849551\n",
      "0.9756563541492477\n",
      "0.02138333201350554\n",
      "0.0213950578450975\n",
      "0.022035185818911607\n",
      "0.02156901082614163\n",
      "0.9815877997234856\n",
      "0.023735865359625123\n",
      "0.021333397345295515\n",
      "0.02147310715179333\n",
      "0.021934880746409983\n",
      "0.9813624596047346\n",
      "0.023357698330250896\n",
      "0.021527289780159747\n",
      "0.9815331335375227\n",
      "0.02119176861085286\n",
      "0.9762706159775981\n",
      "0.9798709757332237\n",
      "0.9801390602243915\n",
      "0.021371195469495376\n",
      "0.021337016540073646\n",
      "0.9807589507081546\n",
      "0.9765496375621189\n",
      "0.977591370949113\n",
      "0.022849943564618582\n",
      "0.02138305975527109\n",
      "0.9809831945257897\n",
      "0.9796671214953513\n",
      "0.9816064960595122\n",
      "0.9805051546816281\n",
      "0.02154258612817629\n",
      "0.02167671806630929\n",
      "0.021111718627861983\n",
      "0.021363563673711762\n",
      "0.02174535110651939\n",
      "0.021666768471334835\n",
      "0.9761077524775458\n",
      "0.02122462891203994\n",
      "0.02118460723340979\n",
      "0.9760588011892234\n",
      "0.021408519690909445\n",
      "0.02202700386794277\n",
      "0.021138789527081973\n",
      "0.021324630453360166\n",
      "0.973277446829279\n",
      "0.9739338058476017\n",
      "0.9810898527578288\n",
      "0.981350490927875\n",
      "0.9789237729480916\n",
      "0.021211675259551498\n",
      "0.9626867366714911\n",
      "0.021171321449703503\n",
      "0.02124150022551611\n",
      "0.02161695903050104\n",
      "0.023388996941328535\n",
      "0.9812236986494852\n",
      "0.9758834390853326\n",
      "0.9785969251503235\n",
      "0.9785120866538629\n",
      "0.9803827990093202\n",
      "0.021373398665140132\n",
      "0.022174722341470804\n",
      "0.9759080243050737\n",
      "0.02147375292296242\n",
      "0.9771977886511856\n",
      "0.9768522392202379\n",
      "0.975627695701931\n",
      "0.9761595156871351\n",
      "0.9781572091800059\n",
      "0.021280830814560576\n",
      "0.021471062649235227\n",
      "0.9806174450963958\n",
      "0.02136964187931799\n",
      "0.022208451666263136\n",
      "0.9809993536361767\n",
      "0.021308988145339602\n",
      "0.021763855871398646\n",
      "0.0215147994474783\n",
      "0.9791259570284627\n",
      "0.021254572876642622\n",
      "0.021379517861226823\n",
      "0.9760088886245937\n",
      "0.975347495802563\n",
      "0.9777209525810912\n",
      "0.021271102339194395\n",
      "0.9631460145236619\n",
      "0.9750340953731579\n",
      "0.9743571328444967\n",
      "0.021317074992869316\n",
      "0.981458807160072\n",
      "0.9790316390343453\n",
      "0.9796078031101546\n",
      "0.9815394161529114\n",
      "0.02112937175662618\n",
      "0.9766885954769065\n",
      "0.9734320230303861\n",
      "0.02152369343328553\n",
      "0.9796822924235302\n",
      "0.9741539852976618\n",
      "0.9820963206913689\n",
      "0.9766250771207328\n",
      "0.981790458453513\n",
      "0.02125726771021172\n",
      "0.02140714533441168\n",
      "0.021325888120677827\n",
      "0.9776217516055825\n",
      "0.9762149017934656\n",
      "0.021849899173633454\n",
      "0.9744507120799216\n",
      "0.021262778226774363\n",
      "0.9796370162532593\n",
      "0.9794003219441854\n",
      "0.021053778797355826\n",
      "0.9799564030613127\n",
      "0.021206562490810883\n",
      "0.02173650013629991\n",
      "0.9819523590732935\n",
      "0.021463185308493712\n",
      "0.021266481175959678\n",
      "0.021856586775975902\n",
      "0.9813606041329006\n",
      "0.9542584939433283\n",
      "0.981557972514666\n",
      "0.021722383011670246\n",
      "0.02156518825405096\n",
      "0.02128925140338426\n",
      "0.021768197413857344\n",
      "0.9744251057616296\n",
      "0.02154489658769028\n",
      "0.021198791635927455\n",
      "0.9796232475604063\n",
      "0.021227881710736404\n",
      "0.9757483367636142\n",
      "0.979533507450387\n",
      "0.9808019512820257\n",
      "0.021270169478943463\n",
      "0.9812413244380241\n",
      "0.9809477112441884\n",
      "0.9791350138642494\n",
      "0.9789297924202929\n",
      "0.9814444329101714\n",
      "0.9765724089787163\n",
      "0.9794438517871793\n",
      "0.021827373228698658\n",
      "0.979434373727252\n",
      "0.02162102127729485\n",
      "0.021125797734112526\n",
      "0.02167280094118185\n",
      "0.021200344727009244\n",
      "0.02142256465834328\n",
      "0.02146555357678436\n",
      "0.9789496601952566\n",
      "0.9805225972480845\n",
      "0.9814295553156578\n",
      "0.021554553531332822\n",
      "0.021328726138354962\n",
      "0.021421701901204425\n",
      "0.9819978077237794\n",
      "0.9806900411852907\n",
      "0.9789550986649064\n",
      "0.021305421206710414\n",
      "0.9742236381396205\n",
      "0.021203743250064487\n",
      "0.96766865717605\n",
      "0.02130331548260857\n",
      "0.022098751538129898\n",
      "0.021225517530642807\n",
      "0.975164726842925\n",
      "0.02153683874269469\n",
      "0.9782270978502918\n",
      "0.02125044498579007\n",
      "0.9782172000145741\n",
      "0.9813567650940772\n",
      "0.02153598825892958\n",
      "0.02158688196990602\n",
      "0.021732059891439944\n",
      "0.021291910861362357\n",
      "0.022383011840595858\n",
      "0.979979277314599\n",
      "0.021180791223929192\n",
      "0.02144023128925175\n",
      "0.021203169723160976\n",
      "0.9804972979041561\n",
      "0.021319845168092773\n",
      "0.981706635301438\n",
      "0.9791098559255713\n",
      "0.9810637709502653\n",
      "0.021653785265934318\n",
      "0.022224026665358165\n",
      "0.9725625465657713\n",
      "0.0214698436520596\n",
      "0.021085607725882773\n",
      "0.963632736584179\n",
      "0.02120856036771063\n",
      "0.022438231981501883\n",
      "0.9797232330547649\n",
      "0.9695012488194293\n",
      "0.021823744082215667\n",
      "0.9785770562271118\n",
      "0.9723316275501203\n",
      "0.979410030116436\n",
      "0.022142375252740987\n",
      "0.022185950190960185\n",
      "0.9740061665745744\n",
      "0.021410321500211153\n",
      "0.9815974534212359\n",
      "0.9810347431263687\n",
      "0.02153993695020057\n",
      "0.9771366196850111\n",
      "0.021597602912490785\n",
      "0.02125106869454626\n",
      "0.9800356654764859\n",
      "0.971781816581713\n",
      "0.9810351894229008\n",
      "0.02133598862098488\n",
      "0.0214210184682451\n",
      "0.9741751083658816\n",
      "0.9814625095567469\n",
      "0.02162891057825415\n",
      "0.02137470500962635\n",
      "0.021296364208247563\n",
      "0.9758914178103041\n",
      "0.9707669729085552\n",
      "0.9792246191474071\n",
      "0.021256193047982562\n",
      "0.021458176029280848\n",
      "0.9811745817863154\n",
      "0.02175304473503757\n",
      "0.9800715042115518\n",
      "0.9811035423447219\n",
      "0.9809427835305953\n",
      "0.9742854400623633\n",
      "0.021507010210829527\n",
      "0.02121251024760645\n",
      "0.021238583647468574\n",
      "0.02244352577950508\n",
      "0.9769767089897264\n",
      "0.9559388659313008\n",
      "0.021575718084901146\n",
      "0.9745835804692924\n",
      "0.02115697533243898\n",
      "0.021574789078044995\n",
      "0.021186619476383065\n",
      "0.9758656676435659\n",
      "0.021195797124129383\n",
      "0.9807565975501554\n",
      "0.9801862500665982\n",
      "0.9817502949021663\n",
      "0.981102129216806\n",
      "0.021289478278224364\n",
      "0.9789421417587765\n",
      "0.9786691731682525\n",
      "0.9778335322255248\n",
      "0.9803995135231786\n",
      "0.9664088784977822\n",
      "0.02127023701664927\n",
      "0.967027974295333\n",
      "0.9785747455879573\n",
      "0.9775992839401252\n",
      "0.9803440635070074\n",
      "0.9759266865459402\n",
      "0.021059896341299733\n",
      "0.9801595090654986\n",
      "0.9810982781073395\n",
      "0.021290093744135326\n",
      "0.9722166662711071\n",
      "0.9778587387942281\n",
      "0.9818427567889869\n",
      "0.021180889014046807\n",
      "0.9720803568220643\n",
      "0.9558115891616247\n",
      "0.02109320284616754\n",
      "0.021101170687874655\n",
      "0.980615875001871\n",
      "0.9767243113526598\n",
      "0.021451869673110115\n",
      "0.021308879812684003\n",
      "0.021328353208930496\n",
      "0.02174731298981002\n",
      "0.9803153939357973\n",
      "0.970741999248779\n",
      "0.9787406571908034\n",
      "0.9732291815941508\n",
      "0.02185813119734294\n",
      "0.02158647792983632\n",
      "0.021345122628202064\n",
      "0.021514294512281852\n",
      "0.021388857408687026\n",
      "0.021486729120738565\n",
      "0.9777509845083568\n",
      "0.022185525148202177\n",
      "0.02310595749897509\n",
      "0.9731499546193768\n",
      "0.02161364211588907\n",
      "0.021323714706190624\n",
      "0.02140870149744092\n",
      "0.9798392975786644\n",
      "0.02170565287945952\n",
      "0.9751243879660847\n",
      "0.02174714270912459\n",
      "0.021894526056938438\n",
      "0.021366700207141206\n",
      "0.021492715164791974\n",
      "0.021497901043475702\n",
      "0.9816982857698614\n",
      "0.9810817845975989\n",
      "0.02177934430677551\n",
      "0.9708300138100973\n",
      "0.9776259337034605\n",
      "0.021159047022548547\n",
      "0.021117489395636246\n",
      "0.9817840380998151\n",
      "0.9737105458716708\n",
      "0.021584250118360675\n",
      "0.021391222402299515\n",
      "0.9807811909464863\n",
      "0.9710294073071797\n",
      "0.9712088426725297\n",
      "0.9800777352405781\n",
      "0.9806620174087364\n",
      "0.021225008435455283\n",
      "0.021216007826753935\n",
      "0.021513828955488076\n",
      "0.9790448013308647\n",
      "0.021206002572980446\n",
      "0.9791807028999272\n",
      "0.02112455437247685\n",
      "0.9794277417811457\n",
      "0.9784378197967516\n",
      "0.9816538421975588\n",
      "0.021380399809577504\n",
      "0.022135204685796127\n",
      "0.021619353507807294\n",
      "0.9759114241685364\n",
      "0.021342419075515266\n",
      "0.9807989033197949\n",
      "0.9793545152341135\n",
      "0.9751849220549286\n",
      "0.021433337831028177\n",
      "0.9807516037332844\n",
      "0.021319475162502724\n",
      "0.9811452809278369\n",
      "0.9797446005948488\n",
      "0.9814250337077206\n",
      "0.9717403331726148\n",
      "0.9813830837813092\n",
      "0.9805627896808823\n",
      "0.9796234287482547\n",
      "0.02149661556054627\n",
      "0.02117981877862017\n",
      "0.021208585948125996\n",
      "0.021497487605655785\n",
      "0.022073108642719125\n",
      "0.9793590260719018\n",
      "0.9807901850011209\n",
      "0.021152479198207355\n",
      "0.021486999495436844\n",
      "0.9792096973685416\n",
      "0.02115389035965501\n",
      "0.9777087600208594\n",
      "0.021519107079383756\n",
      "0.02199750532861851\n",
      "0.971961530973918\n",
      "0.9778394438972924\n",
      "0.9782311184841375\n",
      "0.9819680259174884\n",
      "0.9804865375259443\n",
      "0.021865233849434597\n",
      "0.021634357664393682\n",
      "0.9814794509284911\n",
      "0.9815919317956658\n",
      "0.9818029203924257\n",
      "0.02178349060225045\n",
      "0.9794954152695388\n",
      "0.021205569336129895\n",
      "0.021333855031117933\n",
      "0.9765994161818953\n",
      "0.021925134718973956\n",
      "0.9734468588804434\n",
      "0.980385407256246\n",
      "0.021865149002544368\n",
      "0.9773679026248546\n",
      "0.9671535858117488\n",
      "0.980834648202059\n",
      "0.9804276127870776\n",
      "0.021355365420911952\n",
      "0.021236024181129843\n",
      "0.9704829656347789\n",
      "0.9815790298130728\n",
      "0.9779525298921764\n",
      "0.9816456928575102\n",
      "0.9769556198886106\n",
      "0.021175436684368252\n",
      "0.021793356200542468\n",
      "0.9739386165326956\n",
      "0.021232800408050975\n",
      "0.02120121593901864\n",
      "0.021774561248586084\n",
      "0.021401977229198985\n",
      "0.02131338172500779\n",
      "0.9783017278239913\n",
      "0.021072600649715386\n",
      "0.02111960149390074\n",
      "0.021217092904797057\n",
      "0.978696144659984\n",
      "0.021337637429540677\n",
      "0.9815544578419045\n",
      "0.021585657369144248\n",
      "0.9776258484302586\n",
      "0.021517825392372583\n",
      "0.9816534694828927\n",
      "0.9817648609979016\n",
      "0.9803327660993557\n",
      "0.021397934853018087\n",
      "0.02135971689890803\n",
      "0.9782381313904246\n",
      "0.9811537177930748\n",
      "0.021573823843802784\n",
      "0.9784023150494512\n",
      "0.022182252270371825\n",
      "0.021528917048765632\n",
      "0.9799800833460753\n",
      "0.021802908332261332\n",
      "0.021216278493541834\n",
      "0.9814848971805001\n",
      "0.021987677403700636\n",
      "0.021371611891679146\n",
      "0.9780243221339185\n",
      "0.021131486848321824\n",
      "0.9792734614545685\n",
      "0.977025036198775\n",
      "0.9339147750450022\n",
      "0.021625729827488074\n",
      "0.9806754246846429\n",
      "0.9769088913808935\n",
      "0.9791871333345025\n",
      "0.02117998044024638\n",
      "0.0222900040663884\n",
      "0.9812534204889725\n",
      "0.021668884756173276\n",
      "0.9747939976843867\n",
      "0.9682764712203588\n",
      "0.978253633552061\n",
      "0.021092320872783236\n",
      "0.9700623774449294\n",
      "0.9812298445580225\n",
      "0.9817487813659315\n",
      "0.021192068452094353\n",
      "0.9800200790289518\n",
      "0.02127509065114779\n",
      "0.021546305835387074\n",
      "0.02119219699066025\n",
      "0.02121572546970418\n",
      "0.0212720857378745\n",
      "0.02136144567843331\n",
      "0.021809905440616\n",
      "0.02162717825888999\n",
      "0.980370532096502\n",
      "0.021253471073247997\n",
      "0.9728347796198874\n",
      "0.9807978661439517\n",
      "0.9809635905977778\n",
      "0.02147694831959345\n",
      "0.9796619742091917\n",
      "0.021253726262055586\n",
      "0.9808480589170387\n",
      "0.022087019969279784\n",
      "0.02197575166062078\n",
      "0.02131349346369597\n",
      "0.9794372031036204\n",
      "0.9818717700226315\n",
      "0.021228219711480117\n",
      "0.9806024844809308\n",
      "0.9816130824942275\n",
      "0.9801148655345522\n",
      "0.021458222692489815\n",
      "0.021624014146945737\n",
      "0.021209597531882854\n",
      "0.9785620778381512\n",
      "0.97568198664387\n",
      "0.9806317160913377\n",
      "0.021582394058560304\n",
      "0.9803073083148213\n",
      "0.021341534341401448\n",
      "0.9800156621000873\n",
      "0.9790851916857055\n",
      "0.021847671378490755\n",
      "0.9661173042148931\n",
      "0.02129313721049345\n",
      "0.021656065556342065\n",
      "0.02113859678690904\n",
      "0.9722002697543494\n",
      "0.9807927783892977\n",
      "0.02133891101128969\n",
      "0.021489385002268286\n",
      "0.9808199223026526\n",
      "0.021779613287489227\n",
      "0.021408036602902556\n",
      "0.021112744525030314\n",
      "0.9811417505618559\n",
      "0.02183533288421536\n",
      "0.978925053764527\n",
      "0.9760367244257927\n",
      "0.9782532789884358\n",
      "0.9771424680792163\n",
      "0.022122432708094317\n",
      "0.972820901225408\n",
      "0.02124786173448687\n",
      "0.02121697305126481\n",
      "0.02120837139160958\n",
      "0.02120580633602423\n",
      "0.02110605311598093\n",
      "0.021417215112210296\n",
      "0.021356109367154163\n",
      "0.022207027125506557\n",
      "0.02118134347172826\n",
      "0.021478719062560894\n",
      "0.9733711011390836\n",
      "0.981800675968805\n",
      "0.02152840937016885\n",
      "0.9763307746704494\n",
      "0.021209031769353022\n",
      "0.9811119749413767\n",
      "0.02123923313820006\n",
      "0.021309180987307687\n",
      "0.021156096842105943\n",
      "0.9805256430725278\n",
      "0.021204233898993778\n",
      "0.9801316851190832\n",
      "0.02265648702123942\n",
      "0.9779907451802734\n",
      "0.9803906136103532\n",
      "0.021105310777262524\n",
      "0.9793155855852163\n",
      "0.9811754549129843\n",
      "0.9771648671613764\n",
      "0.0214163080290661\n",
      "0.021262020525623683\n",
      "0.021301572302090097\n",
      "0.9809506569568962\n",
      "0.9806447374544713\n",
      "0.9810197297884872\n",
      "0.02188087611385654\n",
      "0.9780707903947465\n",
      "0.9798541937756939\n",
      "0.9749150487059288\n",
      "0.021623823110437698\n",
      "0.976062026692588\n",
      "0.021261267621143345\n",
      "0.9818378247395457\n",
      "0.9712181272614024\n",
      "0.021298602666414736\n",
      "0.9813348788143267\n",
      "0.021687034342970316\n",
      "0.9809416064354258\n",
      "0.02292511773885804\n",
      "0.021500357933728426\n",
      "0.021148471252911992\n",
      "0.02154861664670694\n",
      "0.021496844041111417\n",
      "0.02115828620410198\n",
      "0.021407249508682502\n",
      "0.021172553213854143\n",
      "0.021189824061839693\n",
      "0.9813949155096484\n",
      "0.022274015873835984\n",
      "0.9690935614649003\n",
      "0.021165143089850985\n",
      "0.021376197463987216\n",
      "0.021227284587660007\n",
      "0.9803287233538371\n",
      "0.021152566854000108\n",
      "0.9810224516792003\n",
      "0.021162631096717892\n",
      "0.979998135514452\n",
      "0.976088332701073\n",
      "0.021143183877018367\n",
      "0.021507115593797498\n",
      "0.980046200682099\n",
      "0.02112988257522755\n",
      "0.02134756762075308\n",
      "0.9790413925394044\n",
      "0.9798122485929697\n",
      "0.980721721428741\n",
      "0.975207707944136\n",
      "0.9771296983377871\n",
      "0.021498181067094297\n",
      "0.9651254192071618\n",
      "0.9719459653828197\n",
      "0.02140741137334041\n",
      "0.02162877918845893\n",
      "0.02121778562654561\n",
      "0.9816468062749272\n",
      "0.9795584890158446\n",
      "0.021327975410400508\n",
      "0.02117213314826017\n",
      "0.9814570223963093\n",
      "0.9802960391744725\n",
      "0.02133031566432341\n",
      "0.021499354110060837\n",
      "0.9745071868286526\n",
      "0.9788832017473866\n",
      "0.021929903784438924\n",
      "0.9803881282138435\n",
      "0.021165019503416364\n",
      "0.02118872706706211\n",
      "0.021776040141109194\n",
      "0.021214902659656313\n",
      "0.9801593870721563\n",
      "0.02129083796208464\n",
      "0.021415702149527933\n",
      "0.021713868591489736\n",
      "0.9772828870158223\n",
      "0.9807648428501273\n",
      "0.021233938555566833\n",
      "0.0216432210584525\n",
      "0.9774356298321016\n",
      "0.9775897505047969\n",
      "0.02142681067471481\n",
      "0.9812132073481535\n",
      "0.021434731976873556\n",
      "0.02133109921799757\n",
      "0.9777711825711481\n",
      "0.021754144155887674\n",
      "0.021390615641991798\n",
      "0.9779689474839907\n",
      "0.9775392666056005\n",
      "0.9787454411804681\n",
      "0.9804967564686546\n",
      "0.021202130638757945\n",
      "0.9812334468165421\n",
      "0.021649371738626716\n",
      "0.9789703265501818\n",
      "0.021174297637397758\n",
      "0.980189743684724\n",
      "0.021882504345482678\n",
      "0.9715686660582299\n",
      "0.021694024216947945\n",
      "0.021682320828484775\n",
      "0.02134644936852251\n",
      "0.021217901206383182\n",
      "0.9781673025647155\n",
      "0.978323941927054\n",
      "0.9815527185768769\n",
      "0.021512377110930715\n",
      "0.021450148448410977\n",
      "0.9807423131734045\n",
      "0.021343407797547807\n",
      "0.021535607100119765\n",
      "0.9813481863059036\n",
      "0.9781916070616542\n",
      "0.9794612870072872\n",
      "0.021282434392620744\n",
      "0.9819153319923422\n",
      "0.9758354253487699\n",
      "0.9691118398869658\n",
      "0.021671851090284522\n",
      "0.9743899059770226\n",
      "0.02134714419781297\n",
      "0.9755614232612451\n",
      "0.021542816001975565\n",
      "0.021568400717191197\n",
      "0.9704491667144816\n",
      "0.9817008577947098\n",
      "0.02150729247997123\n",
      "0.021331619765049492\n",
      "0.9814137642089477\n",
      "0.021196472033962636\n",
      "0.021230985906322268\n",
      "0.021283165847964098\n",
      "0.02178203281738204\n",
      "0.0227331785866986\n",
      "0.9782770856066305\n",
      "0.9803589617915732\n",
      "0.022331526397528643\n",
      "0.021661146183331946\n",
      "0.021685342652765553\n",
      "0.9808777920232993\n",
      "0.021405525666194785\n",
      "0.021118588593640547\n",
      "0.9790212486313252\n",
      "0.021734239343508877\n",
      "0.9634742722692502\n",
      "0.981220934835478\n",
      "0.02157322280109469\n",
      "0.9779974727926294\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=1000, n_features=2, centers=2, random_state=23)\n",
    "# make a model\n",
    "\n",
    "model = Mlp([5],2,['sigmoid','sigmoid'])\n",
    "optim = Optimizer(1,model)\n",
    "n_epochs = 1000\n",
    "for _ in range(n_epochs):\n",
    "    weights_primes = []\n",
    "    bias_primes = []\n",
    "    loss = 0\n",
    "    for j in range(len(X)):\n",
    "        model.set_input(X[j])\n",
    "        y_hats = model()\n",
    "        model.Backward(y[j])\n",
    "        weights_prime = model.getweights_prime()\n",
    "        bias_prime = model.getbias_prime()\n",
    "        weights_primes.append(weights_prime)\n",
    "        bias_primes.append(bias_prime)\n",
    "        loss += model.loss(y[j])\n",
    "    # without np\n",
    "    mean_weights_prime  = mean_w(weights_primes)\n",
    "    mean_bias_primes = mean_b(bias_primes)\n",
    "    model.set_weights_prime(mean_weights_prime)\n",
    "    model.set_bias_prime(mean_bias_primes)\n",
    "    model.update_weights_and_bias(0.5)\n",
    "    cost = loss/len(X)\n",
    "    print(cost)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "accuracy = 0\n",
    "for i in range(len(X)):\n",
    "    model.set_input(X[i])\n",
    "    print(model.forward())\n",
    "    if model.forward() > 0.5:\n",
    "        if y[i] == 1:\n",
    "            accuracy += 1\n",
    "    else:\n",
    "        if y[i] == 0:\n",
    "            accuracy += 1\n",
    "print(accuracy/len(X))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quera1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
